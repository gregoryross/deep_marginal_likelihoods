\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Marginal Likelihoods and Bayes Factors}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Information criteria}{3}}
\newlabel{eq:inf_crit}{{3}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Theoretical results}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bounding the marginal likelihood}{3}}
\newlabel{eq:logmarglike}{{4}{3}}
\newlabel{eq:marg_up}{{5}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}The absolute upper bound to marginal likelihood}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The KL divergence as the number of free parameters}{4}}
\newlabel{eq:upp_low}{{16}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Tightness of bound with Gaussian distributions}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}WIP}{7}}
\@writefile{toc}{\contentsline {paragraph}{A weak lower bound can be established by using Jensen's inequality.}{7}}
\newlabel{eq:lb}{{27}{7}}
\@writefile{toc}{\contentsline {paragraph}{The information gain determines the tightness of the lower bound.}{7}}
\@writefile{toc}{\contentsline {paragraph}{Taking the expectation with respect to the prior is far more informative, however.}{8}}
\@writefile{toc}{\contentsline {paragraph}{An upper bound to the log marginal likelihood is easily calculable from MCMC.}{8}}
\@writefile{toc}{\contentsline {paragraph}{We have tighter upper bound by estimating or bounding the information gain.}{8}}
\@writefile{toc}{\contentsline {paragraph}{Note that I have the usuable inequalities}{8}}
\@writefile{toc}{\contentsline {paragraph}{In the large data limit, this lower bound provides an explanation to Occam's razor.}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A schematic diagram of how the lower bound penalizes model complexity}}{9}}
\newlabel{fig:lb_complexity}{{1}{9}}
\newlabel{eq:asym_like}{{38}{9}}
\@writefile{toc}{\contentsline {paragraph}{Using the re-expressed marginalized likelihood, we can see the entropy forms an upper bound as well.}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}OUTLINE Bayesian linear regression}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Part 1: demonstration}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The upper and lower bounds for Bayesian linear regression .}}{10}}
\newlabel{fig:bayes_linregress}{{2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Part 2: demonstration}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}OUTLINE Model selection with Gaussian mixtures}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}OUTLINE Model selection with neural nets}{11}}
